# üî• Live Benchmark Tracker for IDEs, LLMs, and AI Agents

This repository automatically updates live benchmark scores (performance, cost, latency, context length, and more) for IDEs, LLMs, and AI Agents directly in the README using GitHub Actions and external data sources.

## üìä What Do the Metrics Mean?
- **MMLU (%):** Massive Multitask Language Understanding score (higher is better)
- **HumanEval (%):** Code generation accuracy on HumanEval benchmark (higher is better)
- **Context Length:** Maximum number of tokens the model/IDE/agent can process in a single input
- **Cost ($/1k tokens):** Price per 1,000 tokens processed (lower is cheaper)
- **Startup/Build Time:** How quickly the IDE launches or completes a build
- **Memory Usage:** RAM consumed during typical use (lower is better)
- **CPU Usage:** CPU load during common tasks (lower is better)
- **Project Indexing Time:** Time to index a large project (lower is better)
- **Responsiveness/Latency:** Delay for code completion or UI actions (lower is better)

## üî• LLM Benchmarks (Top 10, Updated Daily)

| Tool/Model     | MMLU (%) | HumanEval (%) | Context Length | Cost ($/1k tokens) |
|----------------|----------|----------------|----------------|--------------------|
| ...Top 10 LLMs dynamically inserted here... |

## üõ†Ô∏è IDE Benchmarks (Top 10, Updated Daily)

| Tool/Model     | Startup/Build Time | Memory Usage | CPU Usage | Project Indexing Time | Responsiveness/Latency | Notes |
|----------------|--------------------|-------------|-----------|----------------------|------------------------|-------|
| ...Top 10 IDEs dynamically inserted here... |

## ü§ñ AI Agent Benchmarks (Top 10, Updated Daily)

| Tool/Model     | MMLU (%) | HumanEval (%) | Context Length | Cost ($/1k tokens) |
|----------------|----------|----------------|----------------|--------------------|
| ...Top 10 Agents dynamically inserted here... |

üïí _Last updated: 2025-06-12 08:00 UTC_

---

This project is open-source and welcomes contributions!
