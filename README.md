# ðŸ”¥ Live Benchmark Tracker for IDEs, LLMs, and AI Agents

This repository automatically updates live benchmark scores (performance, cost, latency, context length) for IDEs, LLMs, and AI Agents directly in the README using GitHub Actions and external data sources.

## ðŸ”¥ Live Benchmarks (Updated Daily)

| Tool/Model     | MMLU (%) | HumanEval (%) | Context Length | Cost ($/1k tokens) |
|----------------|----------|----------------|----------------|--------------------|
| GPT-4-turbo    | 86.5     | 83.0           | 128k           | $0.01              |
| Claude 3 Opus  | 89.1     | 87.5           | 200k           | $0.01              |
| Gemini 1.5 Pro | 87.0     | 86.0           | 1M (streaming) | $0.005             |
| VS Code        | --       | --             | --             | 450ms startup time |

ðŸ•’ _Last updated: 2025-06-12 08:00 UTC_

---

## ðŸ“„ Product Requirements Document (PRD)

See [PRD](./PRD.md) for full details.
